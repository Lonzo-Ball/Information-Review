一.IO概述
    IO的过程 = 等待数据拷贝条件就绪过程 + 数据拷贝过程
    而且在实际的应用场景中，等待消耗的时间往往都高于拷贝的时间。让 IO 更高效，最核心的办法就是让等待的时间尽量少
    低效的IO：等待的比重大
    高效的IO：拷贝的比重大
二.五种IO模型  前 4 中都是同步IO
    1.阻塞IO
	发起系统调用，在内核将数据准备好之前，系统调用会一直等待
	所有的套接字，默认都是阻塞方式
    2.非阻塞IO
	发起系统调用，如果内核还未将数据准备好，系统调用会直接返回，并且返回 EWOULDBLOCK 错误码
	非阻塞IO往往需要程序员以循环的方式反复尝试读/写文件描述符，这个过程称为轮询。这对CPU来说是较大的消耗，一般只有在特定场景下才使用
    3.信号驱动IO
	内核将数据准备好的时候，使用 SIGIO 信号通知应用程序进行数据拷贝操作

	1.建立 SIGIO 信号的信号处理函数，使用系统调用 int sigaction(int signum, const struct sigaction *act,struct sigaction *oldact);
	2.内核将数据准备好之后，递交 SIGIO 信号给信号处理函数
	3.发起系统调用，进行数据拷贝
    4.IO多路转接
	IO多路转接的核心在于能够同时等待多个文件描述符的就绪状态
	同时等待多个文件描述符，将串行化的IO等待并行化，等待时间重叠
    5.异步IO
	内核在数据拷贝完成时，通知应用程序
三.同步通信和异步通信  IO的两个步骤由谁来完成
    1.同步
	在发起一个调用时，没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了
	换句话说，就是由调用者主动等待这个调用的结果
    2.异步
	在发起一个调用时，这个调用就直接返回了，所以没有返回结果
	换句话说，当一个异步过程调用发起时，调用者不会立即得到结果，而是在调用发出后，被调用者通过信号来通知调用者
四.阻塞和非阻塞
    1.阻塞
	阻塞调用是指调用结果返回之前，当前线程会被挂起，调用线程只有在得到结果之后才会返回
    2.非阻塞
	非阻塞调用指在不能立即得到结果之前，该调用不会阻塞当前线程
五.其他高级IO
    非阻塞IO、纪录锁、系统V流机制、I/O多路转接、readv和writev函数以及存储映射IO（mmap），这些统称为高级IO
六.socket就绪条件
    读就绪
	1.socket内核中，接收缓冲区中的字节数，大于等于低水位标记SO_RCVLOWAT，此时可以无阻塞的读该文件描述符，并且返回值大于0
	2.socket TCP通信中，对端关闭连接，此时对该socket读，则返回0
	3.监听的socket上有新的连接请求

	4.socket上有未处理的错误
    写就绪
	1.socket内核中，发送缓冲区中的可用字节数（发送缓冲区的剩余空间的大小），大于等于低水位标记SO_SNDLOWAT，此时可以无阻塞的写，并且返回值大于0
	2.socket的写操作被关闭，对一个写操作被关闭的socket进行写操作，会触发SIGPIPE信号
	3.socket使用非阻塞connect连接成功或失败之后

	4.socket上有未处理的错误
    异常就绪  了解


## IO多路转接之select  ##
系统提供select函数来实现多路复用输入/输出模型
    1.select系统调用是用来让我们的程序监视多个文件描述符的状态变化的
    2.程序会停在select这里等待，至到被监视的文件描述符有一个或多个发生了状态改变

select的函数接口
    #include <sys/select.h>
    #include <sys/time.h>
    #include <sys/types.h>
    #include <unistd.h>
    int select(int nfds, fd_set *readfds, fd_set *writefds,fd_set *exceptfds, struct timeval *timeout);
    参数解释
	nfds：需要监视的最大的文件描述符值+1
	readfds、writefds、exceptfd：分别对应于需要检测的可读文件描述的集合、可写文件描述的集合、异常文件描述符的集合，将描述符添加到哪个集合，就代表这个
	文件描述符监控的是什么就绪条件
	timeout：是结构体timeval，用来设置select的等待时间
    关于timeval结构
	struct timeval {
               time_t         tv_sec;     /* seconds */
               suseconds_t    tv_usec;    /* microseconds */
           };
	用于描述一段时间长度，如果在这段时间内监视的描述符没有事件就绪，则函数返回，返回值为0
    关于timeout的取值
	NULL：select将一直被阻塞，至到某个文件描述符发生状态改变
	0：仅检测描述符集合的状态，然后立即返回，并不等待外部事件的发生
	特定的时间值：如果在指定时间内没有事件发生，select将超时返回
    关于fd_set结构
	其实这个结构就是一个整型数组，更严格的说是一个位图，使用位图中对应的位来表示要监视的文件描述符
	提供了一组fd_set的接口，来比较方便的操作位图
	    void FD_CLR(int fd, fd_set *set);  //用来清除描述符集合set中fd对应的的位
            int  FD_ISSET(int fd, fd_set *set);  //用来测试描述符集合set中fd对应的位是否为真
            void FD_SET(int fd, fd_set *set);  //用来设置描述符集合set中fd对应的位
            void FD_ZERO(fd_set *set);  //用来清楚描述符集合set的全部位
    关于函数返回值
	执行成功则返回文件描述符状态已改变的个数
	如果返回0代表在描述符状态改变前已超过timeout时间

	当有错误发生时返回-1，错误原因存于errno，此时参数readfds、writefds、exceptfds、timeout的值变成不可预测
	errno的可能值有
	    EBADF：文件描述符无效或者该文件已关闭
	    EINTR：此调用被信号所中断
	    EINVAL：参数n为负值
	    ENOMEM：核心内存不足	    

理解select的执行过程
    理解select模型的关键在于理解fd_set，为了说明方便，取fd_set的长度为1字节，fd_set中的每一个比特位可以对应一个文件描述符fd。则1字节长的fd_set最大可对应8
    个fd
	1.执行fd_set set;FD_ZERO(&set);则set为 0000 0000
	2.若fd = 5,执行FD_SET(fd,&set);set变为 0001 0000
	3.若再加入fd = 1，fd = 2，则set变为 0001 0011
	4.执行select(6,&set,0,0,0);
	5.若fd = 1，fd = 2上可读事件就绪，则select返回，此时set变为 0000 0011  没有事件就绪的fd = 5被清空

select的特点
    1.可监控的文件描述符个数取决于sizeof(fd_set)的值，我电脑上sizeof(fd_set) = 128，每一个比特位对应一个文件描述符，则在我电脑上支持的最大文件描述符个数
      是 128*8 = 1024
    2.将fd加入select监控集的同时，还要再使用一个数据结构array保存放到select监控集中的fd
	1.在select返回后，array作为源数据和fdset进行FD_ISSET测试
	2.select返回后会把以前加入的但无事件就绪的fd对应的位清空，则每次开始select前都要重新从array取得fd逐一加入，扫描array的同时取得fd的最大值maxfd，用
	  于设定select的第一个参数

	注意：fd_set的大小可以调整，可能会涉及到重新编译内核

select的缺点
    1.每次调用select，都需要手动设置fd集合，从接口使用角度来说也非常不便
    2.select的描述符是向fd_set集合中添加，fd_set这个结构体决定select最多只能监听1024个描述符（有上限），即select支持的文件描述符数量太小
    3.监控性能随着fd的增多而降低，体现在两个地方
    	1.每次调用select都需要在内核轮询遍历传递进来的所有fd，这个开销在fd很大时也很大
    	2.每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很大时也很大

## IO多路转接之poll ##
poll的函数接口
    #include <poll.h>
    int poll(struct pollfd *fds, nfds_t nfds, int timeout);
    //pollfd 结构体
    struct pollfd {
    	       int   fd;         /* file descriptor */
               short events;     /* requested events */
               short revents;    /* returned events */
    };
    参数解释
	fds：是一个poll函数监听的结构序列，每一个元素包含了三部分内容：
	    fd：文件描述符
	    events：监听的事件集合
	    revents：返回的事件集合
	nfds：表示fds数组的长度
	timeout：表示poll函数的超时时间，单位是毫秒(ms)
    关于events和revents的取值  全都是宏，只有其中一位是1，其它全是0  这里了解就可以，使用时man poll一把就奥克
	      POLLIN  
		     There is data to read.  数据可读（普通数据和优先数据）
              POLLPRI
                     There  is  urgent  data to read (e.g., out-of-band data on TCP socket; pseudoterminal master in packet mode has seen state change in
                     slave).  高优先级数据可读
              POLLOUT
                     Writing now will not block.  数据可写（普通数据和优先数据）
              POLLRDHUP (since Linux 2.6.17)
                     Stream socket peer closed connection, or shut down writing half of connection.  The _GNU_SOURCE feature test macro must  be  defined
                     (before including any header files) in order to obtain this definition.  TCP连接被对方关闭，或者对方关闭了写操作
              POLLERR
                     Error condition (output only).  错误，只能作为输出
              POLLHUP
                     Hang up (output only).  挂起，只能作为输出。比如管道的写段被关闭后，读端描述符上将收到POLLHUP事件
              POLLNVAL
                     Invalid request: fd not open (output only).  文件描述符没有打开，只能作为输出  
       When compiling with _XOPEN_SOURCE defined, one also has the following, which convey no further information beyond the bits listed above:
              POLLRDNORM
                     Equivalent to POLLIN.  普通数据可读
              POLLRDBAND
                     Priority band data can be read (generally unused on Linux).  可以读取优先级波段数据，Linux不支持
              POLLWRNORM
                     Equivalent to POLLOUT.  普通数据可写
              POLLWRBAND
                     Priority data may be written.  可以写入优先级数据
    返回结果
	返回值小于0，表示出错
	返回值等于0，表示poll函数等待超时
	返回值大于0，表示poll由于监听的描述符就绪而返回

poll的优点
    不同与select使用三个位图来表示三个fd_set的方式，poll使用一个pollfd的指针来实现
	1.pollfd结构包含了要监视的events和发生的revents，接口使用比select方便
	2.poll对于监控的描述符并没有最大数量限制

poll的缺点
    poll的监控性能也会随着监控描述符的增多而降低，体现在两个地方
    	1.和select一样，poll需要以轮询遍历fds的方式来获取就绪的描述符
    	2.每次调用poll，都需要把大量的pollfd结构从用户态拷贝到内核态

## IO多路转接之epoll ##
epoll的三个系统调用
    1.epoll_create  创建了一个epoll模型
    	#include <sys/epoll.h>
    	int epoll_create(int size);
    	返回值占一个文件描述符
    	自从Linux2.6.8之后，size参数是被忽略的
    	用完之后必须用close关闭

    2.epoll_ctl  用户告诉操作系统
	int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
        参数解释
	    epfd：epoll的句柄
	    op：表示动作，用三个宏来表示
		EPOLL_CTL_ADD：注册新的 fd 到 epfd 中
		EPOLL_CTL_MOD：修改已经注册的 fd 的监听事件
		EPOLL_CTL_DEL：从 epfd 中删除一个 fd
	
	    fd：需要监听的fd
	    event：告诉内核需要监听什么事

	关于 epoll_event 结构体
	    typedef union epoll_data {
               void        *ptr;
               int          fd;
               uint32_t     u32;
               uint64_t     u64;
            } epoll_data_t;

            struct epoll_event {
               uint32_t     events;      /* Epoll events */
               epoll_data_t data;        /* User data variable */
            };
	    events可以是以下几个宏的集合
	 	EPOLLIN  表示对应的文件描述符可以读（包括对端socket正常关闭）
              		The associated file is available for read(2) operations.
       		EPOLLOUT  表示对应的文件描述符可以写 
              		The associated file is available for write(2) operations.
       		EPOLLRDHUP (since Linux 2.6.17)  
              		Stream socket peer closed connection, or shut down writing half of connection.  (This flag is especially useful for writing simpl
			e code  to detect peer shutdown when using Edge Triggered monitoring.)
       		EPOLLPRI  表示对应的文件描述符有紧急数据可读
              		There is urgent data available for read(2) operations.
       		EPOLLERR  表示对应的文件描述符发生错误
              	Error  condition  happened on the associated file descriptor.  epoll_wait(2) will always wait for this event; it is not necessary to set 
		it in events.
       		EPOLLHUP  表示对应的文件描述符被挂断
              		Hang up happened on the associated file descriptor.  epoll_wait(2) will always wait for this event; it  is  not  necessary  to  s
			et  it  in events.
       		EPOLLET  将epoll设为边缘触发模式，这是相对于水平触发来说的
              		Sets  the Edge Triggered behavior for the associated file descriptor.  The default behavior for epoll is Level Triggered.  See ep
			oll(7) for more detailed information about Edge and Level Triggered event distribution architectures.
      		EPOLLONESHOT (since Linux 2.6.2)  只监听一次事件，当监听完这次事件之后，如果还需要监听这个文件描述符的话，需要再次把这个文件描述符加入到
			epoll队列里
              		Sets the one-shot behavior for the associated file descriptor.  This means that after an event is pulled out with epoll_wait(2) t			 he associated  file  descriptor  is internally disabled and no other events will be reported by the epoll interface.  The user mu
			st call epoll_ctl() with EPOLL_CTL_MOD to rearm the file descriptor with a new event mask.

    3.epoll_wait  收集在epoll监控的事件中已经发生的事件 / 操作系统将已经就绪的文件描述符对应的事件告诉用户
	int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);
	参数解释
	    epfd：epoll的句柄
	    events：分配好的 epoll_event 结构体数组  epoll将会把发生的事件赋值到 events 数组中（events不可以是空指针，内核只负责把数据复制到这个 
	 	    events 数组中，不会去帮助我们在用户态中分配内存）
	    maxevents：告诉内核这个 events 有多大，这个 maxevents 的值不能大于创建 epoll 时的 size
	    timeout：超时时间，单位是毫秒  0会立即返回，-1是永久阻塞
	返回值
	    如果函数调用成功，返回对应 I/O 上已经准备好的文件描述符数目
	    返回 0 表示已将超时
	    返回小于 0 表示函数失败 

epoll工作原理
    当某一进程调用 epoll_craete 方法时，Linux 内核会创建一个 eventpoll 结构体，这个结构体当中有两个成员与 epoll 的工作过程密切相关
    struct eventpoll{ 
    	....      
    	//1.红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件
    	struct rb_root  rbr;

    	2.双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件 
    	struct list_head rdlist;
    	.... 
    };
    每一个 epoll 对象都有一个独立的 eventpoll 结构体，用于存放通过 epoll_ctl 方法向 epoll 对象中添加进来的事件
    这些事件都会挂载在红黑树中，因此，重复添加的事件可以通过红黑树高效的识别出来（红黑树的插入时间效率是 lgN）
    而所有添加到 epoll 中的事件都会与设备（网卡）驱动程序建立回调关系，也就是说，当有就绪事件时会调用这个回调方法
    这个回调方法在内核中叫 callback，它会将就绪的事件添加到 rdlist 双链表中
    在 epoll 中，对于每一个事件，都会建立一个 epitem 结构体
    struct epitem{
    	struct rb_node  rbn;//红黑树节点
    	struct list_head    rdllink;//双向链表节点      
    	struct epoll_filefd  ffd;  //事件句柄信息      
    	struct eventpoll *ep;    //指向其所属的eventpoll对象      
    	struct epoll_event event; //期待发生的事件类型  
    } 
    当调用 epoll_wait 检查是否有事件发生时，只需要检查 eventpoll 对象中的 rdlist 双向链表中是否有 epitem 元素即可
    如果 rdlist 不为空，则把就绪的事件复制到用户态，同时将就绪事件的数目返回给用户。这个操作的时间复杂度是 O(1) 

总结一下，epoll 的使用过程就是三部曲：
    1.调用 epoll_craete 创建一个 epoll 句柄
    2.调用 epoll_ctl 将要监控的文件描述符进行注册
    3.调用 epoll_wait 等待文件描述符就绪

epoll的优点
    1.文件描述符数目无上限：通过 epoll_ctr 来注册一个文件描述符，内核中使用红黑树的数据结构来管理所有需要监控的文件描述符
    2.基于事件的就绪通知方式：一旦被监听的某个文件描述符就绪，内核会采用类似于 callback 的回调机制，迅速激活这个文件描述符，这样随着文件描述符数量的增加
      也不会影响判定就绪的性能
    3.维护就绪队列当文件描述符就绪，就会被放到内核中的一个就绪队列中。这样调用 epoll_wait 获取就需文件描述符的时候，只要取到队列中的元素即可，操作的事件
      复杂度是 O(1)
    4.内存映射机制：内核直接将就绪队列通过 mmap 的方式映射到用户态，避免了拷贝内存这样的额外性能开销

epoll的工作方式  两种
    假设有这样的一个情景：
	我们已经把一个 TCP socket 添加到 epoll 模型中
	这个时候 socket 的另一端写入了 2KB 的数据
	调用 epoll_wait，并且它会返回，说明它已经准备好读取操作
	然后调用 read，只读取了 1KB 的数据
	继续调用 epoll_wait ......
    1.水平触发(LT)  epoll默认状态下就是 LT 工作模式
	当 epoll 检测到 socket 上事件就绪的时候，可以不立刻进行处理，或者只处理一部分
	如上面情形，由于只读取了 1KB 数据，缓冲区中还剩 1KB 数据，在第二次调用 epoll_wait 时，epoll_wait 仍然会立刻返回并通知 socket 读事件就绪
	至到缓冲区中的数据都被处理完，epoll_wait 才不会立刻返回
	支持阻塞读写和非阻塞读写
    2.边缘触发(ET)  
	当 epoll 检测到 socket 上事件就绪时，必须立刻处理
	如上面情形，虽然只读了 1KB 数据，缓冲区中还剩 1KB 数据，第二次调用 epoll_wait 的时候，epoll_wait 不会再返回了
	也就是说，ET模式下，文件描述符上的事件就绪后，只有一次处理机会
	ET的性能比LT的性能高，因为 epoll_wait 返回的次数少了很多  Nginx 默认采用 ET 模式使用 epoll
	select 和 poll 其实也是工作在 LT 模式下，epoll 既可以支持 LT，也可以支持 ET	

	只支持非阻塞读写
	    1.ET 模式下数据就绪只会通知一次，也就是说，如果要使用 ET 模式，当数据就绪时，需要一直 read，至到读完为止
	    2.如果当前 fd 为阻塞，那么当读完缓冲区中的数据时，如果对端没有关闭连接，那么该 read 函数会一直阻塞，影响其他 fd 以及后续逻辑
	    3.所以此时将 fd 设为非阻塞，当没有数据的时候，read 虽然读取不到任何内容，但是肯定不会被hang住
	    4.那么此时，说明缓冲区中的数据已经读取完毕，需要继续处理后续逻辑

epoll 的使用场景
    对于多连接，且多连接中只有一部分连接比较活跃时，比较适合用 epoll
    一个需要处理上万个客户端的服务器，比如各种互联网 APP 的入口服务器，这样的服务器就很适合使用 epoll

epoll 中的惊群问题
    在多线程/多进程环境下，为了提高程序的稳定性，往往会让多个线程/多个进程同时 epoll_wait 监听的socket。当一个新的连接请求到来时，操作系统不知道选派
    那个线程/进程处理此事件，则干脆将其中几个线程/进程给唤醒，而实际上只有其中一个进程/线程能够成功处理新连接，其他线程都将失败，且errno错误码为EAGAIN
    这种现象称为惊群效应，惊群效应肯定会带来资源的消耗和性能的影响

    不建议让多个线程同时epoll_wait 监听的 socket，而是让其中一个线程epoll_wait,当有新的连接请求到来时，由epoll_wait的线程调用accept，建立新的连接，然后
    交给其他工作线程处理后续的数据读写请求，这样就可以避免多线程环境下的惊群问题

    目前很多开源软件，如lighttpd,nginx等都采用master/workers的模式提高软件的吞吐能力及并发能力，在nginx中甚至还采用了负载均衡的技术，在某个子进程的处理
    能力达到一定负载之后，由其他负载较轻的子进程负责epoll_wait的调用，那么nginx和Lighttpd是如何避免epoll_wait的惊群效用的。
    lighttpd的解决思路：无视惊群效应，仍然采用master/workers模式，每个子进程仍然在监听的socket上调用epoll_wait，当有新的连接请求发生时，操作系统仍
    然只是唤醒其中部分的子进程来处理该事件，仍然只有一个子进程能够成功处理此事件，那么其他被惊醒的子进程捕获EAGAIN错误，并无视。
    nginx的解决思路：在同一时刻，永远都只有一个子进程在监听的socket上epoll_wait，其做法是，创建一个全局的pthread_mutex_t，在子进程进行epoll_wait前，则先
    获取锁
